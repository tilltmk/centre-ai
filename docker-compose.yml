version: '3.8'

services:
  # Centre AI MCP Server
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: centre-ai-mcp-server
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - FLASK_HOST=0.0.0.0
      - FLASK_PORT=5000
      - SECRET_KEY=${SECRET_KEY:-change-this-secret-key-in-production}
      - API_KEY=${API_KEY:-dev-api-key-12345}
      - BASIC_AUTH_USERNAME=${BASIC_AUTH_USERNAME:-admin}
      - BASIC_AUTH_PASSWORD=${BASIC_AUTH_PASSWORD:-admin}
      - MEMORY_DB_PATH=/app/mcp_data/memory.db
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
    volumes:
      - ./mcp_data:/app/mcp_data
      - ./logs:/app/logs
    networks:
      - centre-ai-network
    restart: unless-stopped
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis for caching and session storage
  redis:
    image: redis:7-alpine
    container_name: centre-ai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - centre-ai-network
    restart: unless-stopped
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Ollama for local AI models
  ollama:
    image: ollama/ollama:latest
    container_name: centre-ai-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - centre-ai-network
    restart: unless-stopped
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  centre-ai-network:
    driver: bridge

volumes:
  redis-data:
    driver: local
  ollama-data:
    driver: local
